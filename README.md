
# ğŸª™ Projet de scraping CoinMarketCap

> **Formation IA School â€“ Projet Python / Web Scraping**
> Auteur : *Philippe H.*
> Date : *2025*

---

## ğŸ¯ Objectif du projet

Lâ€™objectif de ce projet est de **rÃ©cupÃ©rer automatiquement les donnÃ©es des 300 plus grandes cryptomonnaies** listÃ©es sur [**CoinMarketCap**](https://coinmarketcap.com/), afin dâ€™en extraire :

* leur **nom**,
* leur **prix**,
* leur **capitalisation boursiÃ¨re (market cap)**,
* ainsi que plusieurs **indicateurs avancÃ©s** (FDV, volume, ratio market_cap / FDV).

Le rÃ©sultat est enregistrÃ© dans un fichier JSON sous la forme :

```json
[
  {
    "nom": "Bitcoin",
    "prix": "$64,500",
    "market_cap": "$1,270,000,000,000",
    "stats": {
      "fdv": "$1,350,000,000,000",
      "volume": "$25,000,000,000",
      "ratio": 0.94
    }
  },
  ...
]
```

Enfin, le script trie les cryptomonnaies ayant un **ratio â‰¤ 0.3**, signe dâ€™un potentiel risque de **dilution future** du prix.

---

## ğŸ§  Contexte pÃ©dagogique

Ce projet sâ€™inscrit dans le cadre du module de **Web Scraping et automatisation de la collecte de donnÃ©es**.
Lâ€™objectif Ã©tait de :

* comprendre le fonctionnement du protocole HTTP et des restrictions liÃ©es au scraping (robots.txt, user-agent),
* tester diffÃ©rentes **approches techniques** (requests, headers, Selenium, etc.),
* apprendre Ã  **structurer un projet** avec une architecture claire, orientÃ©e objet, et un **code rÃ©utilisable**.

---

## ğŸ§± Architecture du projet

```
.
â”œâ”€â”€ main.py                    # point d'entrÃ©e du script
â”œâ”€â”€ coinmarketcap_scraper.py   # classe principale CoinMarketCapScraper
â”œâ”€â”€ scraping_bs4.py            # fonctions utilitaires de parsing BeautifulSoup
â”œâ”€â”€ .env                       # variables dâ€™environnement (URL, USER_AGENTâ€¦)
â”œâ”€â”€ .gitignore                 # exclusion des fichiers inutiles (Mac, venv, etc.)
â””â”€â”€ out/                       # rÃ©pertoire de sortie (HTML, JSON)
```

---

## ğŸ§© Structure du code

### 1. `scraping_bs4.py` â€” Parsing et nettoyage HTML

Ce module regroupe toutes les fonctions dâ€™analyse du contenu HTML :

* **`extract_one_crypto()`** â†’ extrait `nom`, `prix`, `market_cap`, `fdv`, `volume`, `ratio` depuis une page dÃ©tail.
* **`money_to_float()`** â†’ convertit des montants comme `$1.25B` ou `$950M` en nombre rÃ©el.
* **`stats_value_box()`** â†’ dÃ©tecte les valeurs dans les blocs `<dt>/<dd>`.
* **`row_to_href()`** â†’ extrait les URLs de chaque cryptomonnaie depuis la liste principale.

â¡ï¸ En clair : câ€™est la â€œboÃ®te Ã  outilsâ€ BeautifulSoup.
Elle ne tÃ©lÃ©charge rien, mais sait **comprendre** le HTML rÃ©cupÃ©rÃ©.

---

### 2. `coinmarketcap_scraper.py` â€” La classe principale

Ce fichier contient la classe **`CoinMarketCapScraper`**, qui orchestre toutes les Ã©tapes du scraping :

#### ğŸ”¹ MÃ©thodes principales :

| MÃ©thode             | Description                                                                                                                                            |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `crawl_list()`      | Utilise **Selenium** pour parcourir la liste principale (scroll/pagination) et sauvegarder le HTML.                                                    |
| `build_details()`   | Lit le fichier de liste, envoie des **requÃªtes `requests`** sur chaque URL de cryptomonnaie, affiche la progression en direct, et concatÃ¨ne les pages. |
| `export_json()`     | Extrait les infos (nom, prix, market cap, FDV, etc.) et les enregistre dans `cryptomonnaie.json`.                                                      |
| `filter_by_ratio()` | Filtre les cryptos avec un ratio market_cap / FDV â‰¤ 0.3 et crÃ©e un fichier `cryptomonnaie_ratio_0.3.json`.                                             |
| `run()`             | Point dâ€™entrÃ©e : exÃ©cute tout le pipeline de bout en bout.                                                                                             |

Cette approche **combine Selenium et requests** :

* Selenium = navigation dynamique pour charger la table JS,
* Requests = rÃ©cupÃ©ration rapide des pages dÃ©tail.

---

### 3. `main.py` â€” Point dâ€™entrÃ©e

Le fichier `main.py` instancie la classe et exÃ©cute le scraping complet :

```python
from coinmarketcap_scraper import CoinMarketCapScraper

if __name__ == "__main__":
    scraper = CoinMarketCapScraper()
    scraper.run(total=300, take=300, threshold=0.3, echo=True)
```

ExÃ©cution :

```bash
python main.py
```

---

## âš™ï¸ BibliothÃ¨ques utilisÃ©es

| Librairie                           | RÃ´le                                         |
| ----------------------------------- | -------------------------------------------- |
| **requests**                        | RequÃªtes HTTP rapides sur les pages dÃ©tail   |
| **selenium**                        | ExÃ©cution du JavaScript pour la page liste   |
| **beautifulsoup4**                  | Parsing HTML et extraction des donnÃ©es       |
| **webdriver-manager**               | Gestion automatique du driver Chrome         |
| **dotenv**                          | Chargement des variables dâ€™environnement     |
| **json / re / os / time / pathlib** | Standard Python (fichiers, regex, I/O, etc.) |

---

## ğŸ” Fichier `.env` (optionnel)

Exemple :

```bash
URL=https://coinmarketcap.com/
USER_AGENT=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)
SAVE_DIR=out
WAIT_TIME=5
WAIT_TIME_MAX=15
```

Ces valeurs sont chargÃ©es automatiquement via `python-dotenv`.

---

## ğŸ§° Installation et exÃ©cution

### 1ï¸âƒ£ Installation de lâ€™environnement

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ Lancer le scraping complet

```bash
python main.py
```

RÃ©sultat :

```
cmc_pages.html
cmc_detail.html
cryptomonnaie.json
cryptomonnaie_ratio_0.3.json
```


---

## ğŸ§© Partie explicative

Voici le rÃ©cap rapide de tout ce quâ€™on a tentÃ© pour CoinMarketCap, pourquoi Ã§a bloquait, et pourquoi la derniÃ¨re approche marche 

# robots.txt
Tout d'abord le fichier robots.txt de CoinMarketCap nous donne la permission de voir les pages publiques de la liste et de dÃ©tail, mais nous devons utiliser un dÃ©guisement (User-Agent) et respecter un dÃ©lai pour ne pas Ãªtre bloquÃ©s par leurs systÃ¨mes de sÃ©curitÃ©.

# 1) `requests` â€œtout simpleâ€, sans en-tÃªtes

* **IdÃ©e** : `requests.get(URL)` puis BeautifulSoup sur le HTML.
* **Ce qui sâ€™est passÃ©** : soit **403 Forbidden**, soit **HTML quasi vide** (pas de `<tbody><tr>`).
* **Pourquoi Ã§a Ã©choue** :

  * **Anti-bot/WAF** : absence de User-Agent crÃ©dible, pas de cookies ni de navigation â€œrÃ©alisteâ€.
  * **Rendu client** : la **liste des cryptos est chargÃ©e en JavaScript** â†’ le HTML initial ne contient pas les lignes du tableau.

# 2) `requests` avec User-Agent minimal / quelques headers

* **IdÃ©e** : mÃªme chose mais en ajoutant un `User-Agent` et 2-3 en-tÃªtes.
* **RÃ©sultat** : un peu mieux, mais **403** frÃ©quents ou **contenu toujours incomplet**.
* **Pourquoi Ã§a Ã©choue** :

  * Les protections comparent plus de signaux (cookies initiaux, ordre des requÃªtes, entÃªtes â€œnavigateurâ€ cohÃ©rents, etc.).
  * MÃªme avec accÃ¨s, **la page liste reste vide sans exÃ©cuter le JS**.

# 3) `requests` + â€œgrosâ€ headers (UA rÃ©aliste, Referer, Accept-Languageâ€¦)

* **IdÃ©e** : se rapprocher dâ€™un navigateur (headers complets).
* **RÃ©sultat** : un peu moins de 403, mais **toujours pas de lignes** dans la page **liste** (DOM rendu cÃ´tÃ© client).
* **Pourquoi Ã§a Ã©choue** :

  * **ProblÃ¨me structurel** (SPA) : sans moteur JS, le HTML ne contiendra pas les donnÃ©es de la table.
  * Les pages **dÃ©tail** sont parfois **partiellement SSR**, mais pas fiable Ã  100% â†’ extraction inconstante.

# 4) Tout faire au navigateur (Playwright/Selenium) â€” â€œplein JSâ€

* **IdÃ©e** : ouvrir **chaque** page (liste + dÃ©tail) avec un navigateur headless, attendre le rendu, puis scraper.
* **RÃ©sultat** : **fonctionne** techniquement, mais **trop lent** et plus fragile (timeouts, ressources CPU/RAM, risque de blocage).
* **Pourquoi câ€™est peu pratique** :

  * DÃ©marrer un moteur pour **des centaines de pages dÃ©tail** coÃ»te cher.
  * Les temps dâ€™attente et de synchronisation (sÃ©lecteurs, `wait_until`, `networkidle`) sâ€™additionnent.

# âœ… 5) MÃ©thode retenue (celle qui marche)

**Selenium pour la liste + `requests` pour les dÃ©tails + BeautifulSoup partout**

**Pipeline**

1. **Selenium (headless)** sur la **page liste** :

   * scroll/pagination jusquâ€™Ã  ~300 lignes,
   * on **sauvegarde le HTML** (ex.: `cmc_pages.html`).
2. On **parse ce HTML** (BeautifulSoup) pour **rÃ©cupÃ©rer les URLs** des cryptos (`/currencies/...`).
3. Pour **chaque URL dÃ©tail**, on envoie une **requÃªte `requests`** (en-tÃªtes rÃ©alistes) â†’ souvent **suffisant** car la page dÃ©tail expose le prix/les stats dans le HTML final (ou des blocs SSR).
4. **BeautifulSoup** extrait :

   * `nom`, `prix`, `market_cap`, et un bloc **`stats`** (FDV, volume 24h, etc.) en sâ€™appuyant sur la **sÃ©mantique `<dt>/<dd>`** quand disponible (plus robuste que les classes volatiles).
5. On **sÃ©rialise** en `cryptomonnaie.json`, puis on **filtre** les entrÃ©es avec **ratio `market_cap / fdv â‰¤ 0.3`** dans un second JSON.

**Pourquoi Ã§a marche mieux**

* **La liste est 100% JS** â†’ on **doit** passer par un navigateur (Selenium) une seule fois pour collecter les liens.
* Les **pages dÃ©tail** sont **souvent assez SSR** pour quâ€™un `requests` + BS4 suffise (rapide, lÃ©ger, scalable).
* On Ã©vite de lancer un navigateur pour chaque dÃ©tail â†’ **gros gain de performance et de stabilitÃ©**.
* Le parsing par **`<dt>/<dd>`** rend le code **rÃ©silient** aux changements de classes CSS.

**Ce quâ€™on a fait pour fiabiliser**

* Headers rÃ©alistes (UA, Referer) cÃ´tÃ© `requests`.
* Attentes explicites cÃ´tÃ© Selenium (prÃ©sence de `<tbody>`, scroll jusquâ€™Ã  stabilisation).
* Fallbacks CSS raisonnables + normalisation des libellÃ©s (FR/EN) pour `market_cap`, `FDV`, `volume`.
* Convertisseurs de montants (`$1.92T`, `$420.5B`, etc.) pour calculer les **ratios**.

---

## En Bref

* Les approches **100% `requests`** Ã©chouent sur la **liste** car **JS** â **HTML vide** ou **403**.
* Les approches **100% navigateur** marchent mais **lentes** et lourdes.
* La solution **hybride** (Selenium pour la **liste**, `requests` pour les **dÃ©tails**) est **la seule** qui a rÃ©uni **fiabilitÃ© + rapiditÃ©**, avec un parsing **robuste** via `<dt>/<dd>` et une sortie JSON propre + filtre FDV.

---

## ğŸ’¬ Conclusion

Ce projet montre que le **scraping nâ€™est pas quâ€™une question de code**, mais surtout dâ€™**analyse de structure web et de stratÃ©gie dâ€™extraction** :
identifier les pages rendues cÃ´tÃ© client, comprendre le JavaScript, gÃ©rer les dÃ©lais, et surtout, **penser robuste et scalable**.

Lâ€™approche retenue â€” Selenium pour la dÃ©couverte, requests pour la collecte, BeautifulSoup pour lâ€™analyse â€” reprÃ©sente un **Ã©quilibre optimal entre performance et fiabilitÃ©**.

---
